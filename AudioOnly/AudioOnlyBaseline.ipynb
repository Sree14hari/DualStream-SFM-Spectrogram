{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb35b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Restored NUM_CLASSES: 6 (['Cysts_Structural' 'Dysarthia' 'Laryngitis' 'Vox senilis' 'parkinson'\n",
      " 'spasmodische_dysphonie'])\n",
      "‚úÖ Data Loaders Ready!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SFM_CSV = \"sfm_features.csv\"\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# --- 2. RESTORE DATA & CLASSES ---\n",
    "full_df = pd.read_csv(SFM_CSV)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "full_df['label_encoded'] = label_encoder.fit_transform(full_df['label_name'])\n",
    "NUM_CLASSES = len(label_encoder.classes_) # <--- This fixes your NameError\n",
    "print(f\"‚úÖ Restored NUM_CLASSES: {NUM_CLASSES} ({label_encoder.classes_})\")\n",
    "\n",
    "# --- 3. RESTORE DATASET CLASS ---\n",
    "class VoicePathologyDataset(Dataset):\n",
    "    def __init__(self, df, sfm_scaler, sfm_cols):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.sfm_scaler = sfm_scaler\n",
    "        self.sfm_cols = sfm_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_path = row['file_path']\n",
    "        # We only need the path for Audio-Only model, but we keep format consistent\n",
    "        raw_sfm = row[self.sfm_cols].values.astype(np.float32)\n",
    "        norm_sfm = self.sfm_scaler.transform([raw_sfm])[0]\n",
    "        sfm_tensor = torch.tensor(norm_sfm, dtype=torch.float32)\n",
    "        label = torch.tensor(row['label_encoded'], dtype=torch.long)\n",
    "        return file_path, sfm_tensor, label\n",
    "\n",
    "# --- 4. RESTORE LOADERS ---\n",
    "sfm_cols = ['jitter_local', 'jitter_rap', 'shimmer_local', 'shimmer_apq3', \n",
    "            'hnr', 'f1', 'f2', 'f3', 'f4', 'f0_mean']\n",
    "scaler = StandardScaler()\n",
    "train_subset = full_df[full_df['split'] == 'train']\n",
    "scaler.fit(train_subset[sfm_cols].values)\n",
    "\n",
    "# Custom Collate (Required for Audio-MAE)\n",
    "def custom_collate(batch):\n",
    "    paths = [item[0] for item in batch] \n",
    "    sfms = torch.stack([item[1] for item in batch])\n",
    "    labels = torch.stack([item[2] for item in batch])\n",
    "    return paths, sfms, labels\n",
    "\n",
    "train_ds = VoicePathologyDataset(full_df[full_df['split'] == 'train'], scaler, sfm_cols)\n",
    "val_ds = VoicePathologyDataset(full_df[full_df['split'] == 'val'], scaler, sfm_cols)\n",
    "test_ds = VoicePathologyDataset(full_df[full_df['split'] == 'test'], scaler, sfm_cols)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "print(\"‚úÖ Data Loaders Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287e2384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéß Loading Audio-MAE (Audio Only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sreeh\\miniconda3\\envs\\your_env_name\\lib\\site-packages\\huggingface_hub\\file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sreeh\\miniconda3\\envs\\your_env_name\\lib\\site-packages\\huggingface_hub\\file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting Audio-Only Baseline Training...\n",
      "Epoch 1/15 | Loss: 1.6823 | Val Acc: 0.5216\n",
      "Epoch 2/15 | Loss: 1.6010 | Val Acc: 0.5529\n",
      "Epoch 3/15 | Loss: 1.5552 | Val Acc: 0.5769\n",
      "Epoch 4/15 | Loss: 1.5060 | Val Acc: 0.5865\n",
      "Epoch 5/15 | Loss: 1.4747 | Val Acc: 0.5865\n",
      "Epoch 6/15 | Loss: 1.4379 | Val Acc: 0.5938\n",
      "Epoch 7/15 | Loss: 1.4125 | Val Acc: 0.5986\n",
      "Epoch 8/15 | Loss: 1.3877 | Val Acc: 0.6010\n",
      "Epoch 9/15 | Loss: 1.3613 | Val Acc: 0.6106\n",
      "Epoch 10/15 | Loss: 1.3413 | Val Acc: 0.6130\n",
      "Epoch 11/15 | Loss: 1.3241 | Val Acc: 0.6202\n",
      "Epoch 12/15 | Loss: 1.3074 | Val Acc: 0.6274\n",
      "Epoch 13/15 | Loss: 1.2858 | Val Acc: 0.6130\n",
      "Epoch 14/15 | Loss: 1.2675 | Val Acc: 0.6514\n",
      "Epoch 15/15 | Loss: 1.2559 | Val Acc: 0.6538\n",
      "\n",
      "üèÜ Best Audio-Only Accuracy: 65.38%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- MODEL DEFINITION ---\n",
    "class AudioOnlyModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        print(\"üéß Loading Audio-MAE (Audio Only)...\")\n",
    "        self.audio_encoder = AutoModel.from_pretrained(\n",
    "            \"hance-ai/audiomae\", \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.hidden_size = 768 \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio_paths):\n",
    "        audio_feats_list = []\n",
    "        for path in audio_paths:\n",
    "            feat = self.audio_encoder(path) \n",
    "            audio_feats_list.append(feat)\n",
    "            \n",
    "        audio_feats = torch.stack(audio_feats_list)\n",
    "        device = self.classifier[0].weight.device\n",
    "        audio_feats = audio_feats.to(device)\n",
    "        \n",
    "        # Global Average Pooling (Collapse 8x64 grid -> 1 vector)\n",
    "        audio_emb = audio_feats.mean(dim=(2, 3)) \n",
    "        return self.classifier(audio_emb)\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "AUDIO_ONLY_LR = 2e-5\n",
    "EPOCHS = 15\n",
    "\n",
    "audio_model = AudioOnlyModel(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(audio_model.parameters(), lr=AUDIO_ONLY_LR, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nüî• Starting Audio-Only Baseline Training...\")\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    audio_model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for paths, sfms, labels in train_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = audio_model(paths)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Validation\n",
    "    audio_model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for paths, sfms, labels in val_loader:\n",
    "            labels = labels.to(DEVICE)\n",
    "            logits = audio_model(paths)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    val_acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss/len(train_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(audio_model.state_dict(), \"baseline_audio_only.pth\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Audio-Only Accuracy: {best_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b819bc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Evaluating Audio-Only Model on Test Set...\n",
      "\n",
      "üìä Audio-Only Baseline Results:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Cysts_Structural       0.29      0.09      0.14        22\n",
      "             Dysarthia       0.70      0.71      0.71        42\n",
      "            Laryngitis       0.25      0.43      0.32        42\n",
      "           Vox senilis       0.46      0.41      0.43        93\n",
      "             parkinson       0.78      0.84      0.81       200\n",
      "spasmodische_dysphonie       0.00      0.00      0.00        20\n",
      "\n",
      "              accuracy                           0.61       419\n",
      "             macro avg       0.41      0.41      0.40       419\n",
      "          weighted avg       0.59      0.61      0.59       419\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sreeh\\miniconda3\\envs\\your_env_name\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\sreeh\\miniconda3\\envs\\your_env_name\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\sreeh\\miniconda3\\envs\\your_env_name\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load Best Baseline\n",
    "audio_model.load_state_dict(torch.load(\"baseline_audio_only.pth\"))\n",
    "audio_model.eval()\n",
    "\n",
    "print(\"üöÄ Evaluating Audio-Only Model on Test Set...\")\n",
    "\n",
    "# Use Test Loader (defined in previous steps)\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for paths, sfms, labels in test_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        logits = audio_model(paths)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print Report\n",
    "print(f\"\\nüìä Audio-Only Baseline Results:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your_env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
